# HuggingFace LLM-as-a-Judge Evaluation

This directory contains an implementation of the LLM-as-a-Judge evaluation methodology using HuggingFace's inference client and datasets. This approach uses a large language model to evaluate the quality of AI-generated responses.

## Overview

The `llm_as_judge.py` script implements a sophisticated evaluation system where one LLM (the "judge") evaluates the quality of responses generated by another AI system. This methodology is particularly useful for:

- Evaluating AI assistant responses
- Quality assessment without human annotators
- Scalable evaluation of large datasets
- Model comparison and benchmarking

## Key Features

### 1. LLM-as-a-Judge Methodology
- Uses a powerful LLM (Mixtral-8x7B-Instruct) as an evaluator
- Structured evaluation prompts for consistent scoring
- Numerical rating system (1-4 scale) with detailed rationale
- Extensible framework for different evaluation criteria

### 2. Two Evaluation Prompts
- **Basic Judge Prompt**: Simple 0-10 scale evaluation
- **Improved Judge Prompt**: Detailed 1-4 scale with evaluation rationale

### 3. Automated Score Extraction
- Regex-based score extraction from LLM responses
- Error handling for malformed responses
- Structured output format

## Script Breakdown

### LLM Client Initialization
```python
def init_llm_client(model_id="mistralai/Mixtral-8x7B-Instruct-v0.1"):
    return InferenceClient(
        model=model_id,
        timeout=120,
    )
```
- **Purpose**: Sets up connection to HuggingFace inference API
- **Model**: Uses Mixtral-8x7B-Instruct by default (can be customized)
- **Timeout**: 120 seconds for response generation

### Evaluation Prompts

#### Basic Judge Prompt
```python
BASIC_JUDGE_PROMPT = """
You will be given a user_question and system_answer couple.
Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.
Give your answer as a float on a scale of 0 to 10...
"""
```
- **Scale**: 0-10 (continuous)
- **Focus**: Overall helpfulness assessment
- **Output**: Numerical score only

#### Improved Judge Prompt
```python
IMPROVED_JUDGE_PROMPT = """
You will be given a user_question and system_answer couple.
Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.
Give your answer on a scale of 1 to 4...
"""
```
- **Scale**: 1-4 (discrete)
- **Focus**: Detailed evaluation with rationale
- **Output**: Numerical score + evaluation reasoning

### Score Extraction
```python
def extract_judge_score(answer: str, split_str: str = "Total rating:") -> float:
    """Extract the numerical score from the LLM judge's response."""
```
- **Purpose**: Parses LLM response to extract numerical score
- **Method**: Regex-based extraction
- **Error Handling**: Returns None for malformed responses

### Main Evaluation Function
```python
def evaluate_with_llm_judge(questions_answers, llm_client, prompt_template=IMPROVED_JUDGE_PROMPT):
    """Evaluate a list of question-answer pairs using the LLM judge."""
```
- **Input**: List of question-answer pairs
- **Process**: Iterates through pairs, gets LLM evaluation, extracts scores
- **Output**: Pandas DataFrame with results

## Setup and Installation

### Prerequisites
- Python 3.8+
- HuggingFace account with API access
- Internet connection for model inference

### Installation
```bash
# Install required packages
pip install huggingface-hub datasets pandas tqdm

# Set up HuggingFace token (optional, for private models)
export HUGGINGFACE_TOKEN=your-token-here
```

### Running the Script
```bash
# Run the evaluation script
python llm_as_judge.py
```

## Expected Output

The script outputs a structured evaluation with:
- Original question and answer
- LLM judge's complete response
- Extracted numerical score

```
Evaluation Results:
                                                question  llm_judge_score                                    llm_judge_response
0  What is the capital of France?                      4.0  Feedback:::\nEvaluation: The system_answer is e...
1  How do I make a chocolate cake?                     1.0  Feedback:::\nEvaluation: The system_answer is t...
```

## Configuration

### Customizing the Judge Model
```python
# Use a different model
llm_client = init_llm_client("meta-llama/Llama-2-70b-chat-hf")

# Use a smaller, faster model
llm_client = init_llm_client("microsoft/DialoGPT-medium")
```

### Customizing Evaluation Criteria
```python
# Create custom evaluation prompt
CUSTOM_PROMPT = """
Evaluate the following response based on accuracy, helpfulness, and clarity.
Scale: 1-5 (1=poor, 5=excellent)

Question: {question}
Answer: {answer}

Rating: """

# Use custom prompt
results = evaluate_with_llm_judge(qa_pairs, llm_client, CUSTOM_PROMPT)
```

### Batch Processing
```python
# Load dataset from HuggingFace
dataset = load_dataset("your-dataset-name")

# Convert to required format
qa_pairs = [
    {"question": item["question"], "answer": item["answer"]}
    for item in dataset["test"]
]

# Evaluate entire dataset
results = evaluate_with_llm_judge(qa_pairs, llm_client)
```

## Use Cases

### 1. AI Assistant Evaluation
- Evaluate chatbot responses
- Assess customer service AI quality
- Monitor response consistency

### 2. Model Comparison
```python
# Compare different models
models = ["model-a", "model-b", "model-c"]
results = {}

for model in models:
    llm_client = init_llm_client(model)
    results[model] = evaluate_with_llm_judge(qa_pairs, llm_client)
```

### 3. Dataset Quality Assessment
- Evaluate synthetic data quality
- Assess training data relevance
- Monitor data drift

### 4. A/B Testing
- Compare different response strategies
- Evaluate prompt engineering changes
- Test different model configurations

## Advanced Features

### Multi-Criteria Evaluation
```python
def evaluate_multiple_criteria(qa_pair, llm_client):
    criteria = ["accuracy", "helpfulness", "clarity", "relevance"]
    results = {}
    
    for criterion in criteria:
        prompt = f"Evaluate the response for {criterion}..."
        response = llm_client.text_generation(prompt)
        results[criterion] = extract_judge_score(response)
    
    return results
```

### Confidence Scoring
```python
def evaluate_with_confidence(qa_pair, llm_client):
    # Get multiple evaluations and calculate confidence
    evaluations = []
    for _ in range(3):
        response = llm_client.text_generation(prompt)
        score = extract_judge_score(response)
        evaluations.append(score)
    
    return {
        "mean_score": np.mean(evaluations),
        "std_score": np.std(evaluations),
        "confidence": 1 - np.std(evaluations) / np.mean(evaluations)
    }
```

## Performance Optimization

### Caching
```python
import pickle

def cached_evaluation(qa_pairs, cache_file="evaluation_cache.pkl"):
    # Load cached results
    if os.path.exists(cache_file):
        with open(cache_file, "rb") as f:
            return pickle.load(f)
    
    # Run evaluation
    results = evaluate_with_llm_judge(qa_pairs, llm_client)
    
    # Cache results
    with open(cache_file, "wb") as f:
        pickle.dump(results, f)
    
    return results
```

### Parallel Processing
```python
from concurrent.futures import ThreadPoolExecutor

def parallel_evaluation(qa_pairs, max_workers=4):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(evaluate_single_qa, qa_pair, llm_client)
            for qa_pair in qa_pairs
        ]
        results = [future.result() for future in futures]
    return results
```

## Troubleshooting

### Common Issues

1. **Timeout Errors**
   ```python
   # Increase timeout
   llm_client = InferenceClient(model=model_id, timeout=300)
   ```

2. **Rate Limiting**
   ```python
   import time
   
   # Add delays between requests
   for qa_pair in qa_pairs:
       result = llm_client.text_generation(prompt)
       time.sleep(1)  # 1 second delay
   ```

3. **Score Extraction Failures**
   ```python
   # Use more robust extraction
   def robust_score_extraction(response):
       # Try multiple patterns
       patterns = [r"Total rating:\s*(\d+(?:\.\d+)?)", r"Rating:\s*(\d+(?:\.\d+)?)"]
       for pattern in patterns:
           match = re.search(pattern, response)
           if match:
               return float(match.group(1))
       return None
   ```

### Best Practices

- **Prompt Engineering**: Test different prompts for consistency
- **Model Selection**: Choose appropriate model size for your use case
- **Batch Size**: Process in manageable batches to avoid timeouts
- **Error Handling**: Implement robust error handling for production use
- **Validation**: Cross-validate with human evaluations

## References

- [HuggingFace Inference API](https://huggingface.co/docs/api-inference)
- [HuggingFace Datasets](https://huggingface.co/docs/datasets)
- [LLM-as-a-Judge Paper](https://arxiv.org/abs/2306.05685)
- [Mixtral Model](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
